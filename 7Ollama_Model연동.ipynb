{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dcfdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"http://127.0.0.1:11434\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b981ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "('<think>\\n'\n",
      " \"Okay, so I'm trying to understand what LangChain is. From the definition \"\n",
      " \"provided, it looks like it's a framework that brings together language \"\n",
      " 'models with other components of machine learning pipelines. That sounds '\n",
      " 'pretty broad, but let me break it down.\\n'\n",
      " '\\n'\n",
      " 'First, I know that language models are systems that can generate text based '\n",
      " \"on some input. They're used in tasks like text translation, summarization, \"\n",
      " 'and even creative writing. So LangChain must be something that uses these '\n",
      " 'models in a more integrated way with other machine learning tools.\\n'\n",
      " '\\n'\n",
      " 'The framework includes components like data pre-processing, feature '\n",
      " 'engineering, model training, inference, evaluation, deployment, and '\n",
      " 'post-processing. That makes sense because any machine learning pipeline '\n",
      " 'would need various steps to process data, build models, make predictions, '\n",
      " 'assess results, etc.\\n'\n",
      " '\\n'\n",
      " 'I wonder how LangChain specifically integrates these components with '\n",
      " 'language models. For example, does it handle the entire pipeline from data '\n",
      " 'loading through model training and inference all in one place? Or does it '\n",
      " 'separate these tasks for better control?\\n'\n",
      " '\\n'\n",
      " \"Also, I'm curious about the benefits of using a single framework like \"\n",
      " 'LangChain over integrating different tools separately. Maybe it allows for '\n",
      " 'easier customization, better resource management, or more efficient '\n",
      " 'deployment across different environments.\\n'\n",
      " '\\n'\n",
      " 'I should consider where someone might use LangChain. Probably developers '\n",
      " 'working on NLP projects who want to leverage existing language models '\n",
      " 'without re-inventing them. It could also be used in more complex '\n",
      " 'applications that require a broader range of machine learning tasks.\\n'\n",
      " '\\n'\n",
      " \"However, I'm not sure how widely supported LangChain is or if there are any \"\n",
      " 'limitations to its capabilities. Maybe some challenges like dependency '\n",
      " 'management or handling large-scale data across different platforms could '\n",
      " 'pose issues.\\n'\n",
      " '\\n'\n",
      " 'Overall, I think LangChain is a valuable tool for anyone looking to work '\n",
      " 'with language models in a comprehensive and efficient way within machine '\n",
      " 'learning pipelines. It offers flexibility and integration potential, but it '\n",
      " 'might also have some underlying complexities that require more expertise to '\n",
      " 'fully utilize.\\n'\n",
      " '</think>\\n'\n",
      " '\\n'\n",
      " 'LangChain is a comprehensive framework designed to integrate language models '\n",
      " 'into machine learning workflows seamlessly. It combines various components '\n",
      " 'of machine learning, such as data pre-processing, feature engineering, model '\n",
      " 'training, inference, evaluation, deployment, and post-processing, all within '\n",
      " 'one cohesive system. This integration allows users to efficiently manage the '\n",
      " 'entire pipeline from data handling through model training and inference.\\n'\n",
      " '\\n'\n",
      " 'Key features include flexibility for customization, better resource '\n",
      " 'management, and efficient deployment across different environments. '\n",
      " 'LangChain is beneficial for developers working on NLP projects looking to '\n",
      " 'leverage existing models without re-inventing them, as well as those '\n",
      " 'tackling complex applications requiring a broader range of machine learning '\n",
      " 'tasks.\\n'\n",
      " '\\n'\n",
      " 'While widely supported, challenges like dependency management and handling '\n",
      " 'large-scale data may require more expertise. Overall, LangChain offers '\n",
      " 'flexibility and integration potential but presents some complexities that '\n",
      " 'necessitate further exploration for full utilization.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pprint import pprint\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ deepseek-r1:1.5b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"What is LangChain?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(type(response))\n",
    "pprint(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "629ace4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking, \"What is Python?\" and I need to provide a detailed answer. Let me start by recalling the basics of Python.\n",
      "\n",
      "First, Python is a programming language. It's known for being easy to read and write, which makes it popular among beginners. I should mention its design philosophy, like the \"Readability Counts\" motto. That's a key point.\n",
      "\n",
      "Next, I should talk about its features. It's interpreted, which means it's not compiled like C++. Also, it's dynamically typed, so variables don't need explicit types. Oh, and it's garbage-collected, which helps manage memory automatically.\n",
      "\n",
      "Then, the standard library is important. Mentioning modules like os, sys, math, and how they help with common tasks. Also, the Python interpreter and how it's used in different environments.\n",
      "\n",
      "I should note that Python is used in various fields: web development (Django, Flask), data science (Pandas, NumPy), machine learning (Scikit-learn, TensorFlow), and automation. Maybe give examples like scripting, APIs, and scientific computing.\n",
      "\n",
      "The user might be a beginner, so I should keep the explanation simple. Avoid technical jargon unless necessary. Also, highlight its versatility and community support. Maybe mention the Python ecosystem and the large community.\n",
      "\n",
      "Wait, did I cover all the main points? Let me check: definition, features, usage areas, community, and maybe some examples. Yes, that seems comprehensive. Make sure to structure it clearly with headings for each section. Avoid markdown, but the answer should be in a natural, conversational tone.\n",
      "</think>\n",
      "\n",
      "íŒŒì´ì¬ì€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì¤‘ í•˜ë‚˜ë¡œ, \"Readability Counts\"ë¼ëŠ” ì„¤ê³„ ì›ì¹™ì— ë”°ë¼ ì½”ë“œë¥¼ ì‰½ê²Œ ì½ê³  ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ì–¸ì–´ëŠ” ê°„ê²°í•˜ê³  ì§ê´€ì ì¸ ë¬¸ë²•ì„ íŠ¹ì§•ìœ¼ë¡œ, ì²˜ìŒ ë°°ìš°ëŠ” ì‚¬ëŒë„ ì‰½ê²Œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "### ì£¼ìš” íŠ¹ì§•\n",
      "1. **í•´ë‹¹ ì–¸ì–´**  \n",
      "   - í•´ì„ ì–¸ì–´(Interpreter)ë¡œ, ì»´íŒŒì¼ ì—†ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.  \n",
      "   - ë™ê¸° íƒ€ì…(ë™ì  íƒ€ì…)ìœ¼ë¡œ, ë³€ìˆ˜ì˜ íƒ€ì…ì´ ë¯¸ë¦¬ ì •ì˜ë˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.  \n",
      "   - ìë™ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê´€ë¦¬( garbage collection )ì„ ì²˜ë¦¬í•´ ì£¼ì–´, ê°œë°œìê°€ ë³µì¡í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë¬¸ì œë¥¼ ê±±ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.\n",
      "\n",
      "2. **ì‚¬ìš© ê°€ëŠ¥í•œ ë¶„ì•¼**  \n",
      "   - **ì›¹ ê°œë°œ**: Django, Flask ë“±  \n",
      "   - **ë°ì´í„° ê³¼í•™**: Pandas, NumPy, Matplotlib  \n",
      "   - **ë¨¸ì‹ ëŸ¬ë‹**: Scikit-learn, TensorFlow, PyTorch  \n",
      "   - **ìë™í™”**: ì‹œìŠ¤í…œ ê´€ë¦¬, ìŠ¤í¬ë¦½íŒ…  \n",
      "   - **ê¸°íƒ€**: ì• ìì¼ ê°œë°œ, API ê°œë°œ, ì‹œë®¬ë ˆì´ì…˜ ë“±\n",
      "\n",
      "3. **ë¼ì´ë¸ŒëŸ¬ë¦¬**  \n",
      "   - **os**: ìš´ì˜ì²´ì œ ê´€ë ¨ ì‘ì—…  \n",
      "   - **sys**: ì‹œìŠ¤í…œ ì •ë³´ ì ‘ê·¼  \n",
      "   - **math**: ìˆ˜í•™ ê´€ë ¨ í•¨ìˆ˜  \n",
      "   - **datetime**: ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬  \n",
      "   - **requests**: HTTP ìš”ì²­ ì²˜ë¦¬ ë“±\n",
      "\n",
      "### ì‚¬ìš© ì˜ˆì‹œ\n",
      "```python\n",
      "# ë‹¨ìˆœí•œ ê³„ì‚°\n",
      "print(2 + 3)  # 5\n",
      "\n",
      "# ë¦¬ìŠ¤íŠ¸ì™€ ë°˜ë³µ\n",
      "numbers = [1, 2, 3]\n",
      "for num in numbers:\n",
      "    print(num)\n",
      "\n",
      "# í•¨ìˆ˜ ì •ì˜\n",
      "def greet(name):\n",
      "    return f\"Hello, {name}!\"\n",
      "\n",
      "print(greet(\"Alice\"))\n",
      "```\n",
      "\n",
      "### ì£¼ìš” í”„ë ˆì„ì›Œí¬\n",
      "- **Django**: ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ  \n",
      "- **Flask**: ê°„ë‹¨í•œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ  \n",
      "- **NumPy**: ìˆ˜ì¹˜ì  ê³„ì‚°  \n",
      "- **TensorFlow/PyTorch**: ë¨¸ì‹ ëŸ¬ë‹  \n",
      "- **Pandas**: ë°ì´í„° ë¶„ì„\n",
      "\n",
      "### ì»¤ë®¤ë‹ˆí‹°ì™€ í™•ì¥ì„±\n",
      "íŒŒì´ì¬ì€ ëŒ€ê·œëª¨ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™•ì¥ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. ë¼ì´ë¸ŒëŸ¬ë¦¬(ì˜ˆ: Django, NumPy)ëŠ” ìˆ˜ë°±ë§Œ ê°œ ì´ìƒ ì¡´ì¬í•˜ë©°, í”„ë¡œì íŠ¸ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "### ê²°ë¡ \n",
      "íŒŒì´ì¬ì€ ì½”ë“œì˜ ì½ê¸° ì‰¬ì›€, ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë˜ëŠ” ê°•ë ¥í•œ ì–¸ì–´ë¡œ, ê°œë°œìë“¤ì´ ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œí•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤. ğŸŒŸ\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Ollamaë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì‹¤í–‰ ì¤‘ì¸ qwen3:1.7b ëª¨ë¸ ë¡œë“œ\n",
    "llm = ChatOllama(model=\"qwen3:1.7b\")\n",
    "\n",
    "# ë” ì •í™•í•œ ì‘ë‹µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an AI assistant that provides accurate and detailed answers.\"),\n",
    "    (\"human\", \"Q: {question}\\nA:\")\n",
    "])\n",
    "\n",
    "# ìµœì‹  LangChain ë°©ì‹: RunnableSequence í™œìš©\n",
    "chain = prompt_template | llm\n",
    "\n",
    "# ì‹¤í–‰ ì˜ˆì‹œ\n",
    "question = \"íŒŒì´ì¬ì€ ë¬´ì—‡ì¸ê°€ìš”?\"\n",
    "response = chain.invoke({\"question\": question})\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f2c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "First, I need to compare the two numbers: 9.9 and 9.11.\n",
      "\n",
      "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places by adding a trailing zero to 9.9. This gives me 9.90.\n",
      "\n",
      "Now, both numbers are 9.90 and 9.11.\n",
      "\n",
      "Comparing the whole number part first: Both have 9 in the units place, so they are equal there.\n",
      "\n",
      "Next, I'll compare the tenths place. In 9.90, the tenths digit is 9, while in 9.11, it's 1. Since 9 is greater than 1, 9.90 (or 9.9) is larger than 9.11.\n",
      "\n",
      "Therefore, 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step:\n",
      "\n",
      "1. **Express Both Numbers with the Same Decimal Places:**\n",
      "   \n",
      "   - **9.9** can be written as **9.90** to match the two decimal places of **9.11**.\n",
      "   \n",
      "2. **Compare the Whole Number Part First:**\n",
      "   \n",
      "   - Both numbers have **9** in the units place, so they are equal here.\n",
      "\n",
      "3. **Compare the Tenths Place:**\n",
      "   \n",
      "   - In **9.90**, the tenths digit is **9**.\n",
      "   - In **9.11**, the tenths digit is **1**.\n",
      "   \n",
      "   Since **9 > 1**, **9.90 (or 9.9)** is larger than **9.11**.\n",
      "\n",
      "4. **Conclusion:**\n",
      "   \n",
      "   \\[\n",
      "   \\boxed{9.9}\n",
      "   \\]"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "deepseek = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0.5)\n",
    "\n",
    "answer = []\n",
    "for chunk in deepseek.stream(\"which is bigger between 9.9 and 9.11?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "783b3223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "First, I need to compare the two numbers: 9.9 and 9.11.\n",
       "\n",
       "To make an accurate comparison, it's helpful to express both numbers with the same number of decimal places by adding a trailing zero to 9.9. This gives me 9.90.\n",
       "\n",
       "Now, both numbers are 9.90 and 9.11.\n",
       "\n",
       "Comparing the whole number part first: Both have 9 in the units place, so they are equal there.\n",
       "\n",
       "Next, I'll compare the tenths place. In 9.90, the tenths digit is 9, while in 9.11, it's 1. Since 9 is greater than 1, 9.90 (or 9.9) is larger than 9.11.\n",
       "\n",
       "Therefore, 9.9 is bigger than 9.11.\n",
       "</think>\n",
       "\n",
       "To determine which number is larger between **9.9** and **9.11**, let's compare them step by step:\n",
       "\n",
       "1. **Express Both Numbers with the Same Decimal Places:**\n",
       "   \n",
       "   - **9.9** can be written as **9.90** to match the two decimal places of **9.11**.\n",
       "   \n",
       "2. **Compare the Whole Number Part First:**\n",
       "   \n",
       "   - Both numbers have **9** in the units place, so they are equal here.\n",
       "\n",
       "3. **Compare the Tenths Place:**\n",
       "   \n",
       "   - In **9.90**, the tenths digit is **9**.\n",
       "   - In **9.11**, the tenths digit is **1**.\n",
       "   \n",
       "   Since **9 > 1**, **9.90 (or 9.9)** is larger than **9.11**.\n",
       "\n",
       "4. **Conclusion:**\n",
       "   \n",
       "   \\[\n",
       "   \\boxed{9.9}\n",
       "   \\]"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "215d9f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so the question is asking which is bigger between 9.9 and 9.11. Let me think. Both numbers are decimals, right? So they have the same whole number part, which is 9. Then the decimal parts are 0.9 and 0.11. \n",
      "\n",
      "Hmm, I need to compare the decimal parts. Let's break it down. The first number is 9.9, which is 9 + 0.9. The second number is 9.11, which is 9 + 0.11. So the difference between the two decimal parts is 0.9 versus 0.11. \n",
      "\n",
      "Wait, 0.9 is larger than 0.11 because 0.9 is 9 tenths, and 0.11 is 11 hundredths. So 0.9 is definitely bigger than 0.11. Therefore, 9.9 is bigger than 9.11. \n",
      "\n",
      "But let me double-check to make sure I didn't make a mistake. Sometimes when comparing decimals, people might confuse the places. Let's write them out:\n",
      "\n",
      "9.9 is the same as 9.90. So comparing 9.90 and 9.11. The first number has 9 in the units place, 9 in the tenths, and 0 in the hundredths. The second number has 9 in the units, 1 in the tenths, and 1 in the hundredths. \n",
      "\n",
      "So looking at the tenths place: 9 vs 1. Since 9 is greater than 1, the first number is larger. Even if the hundredths place is different, the tenths place already determines the comparison. So yeah, 9.9 is bigger than 9.11. \n",
      "\n",
      "I think that's right. There's no need to go further because the tenths place already shows the difference. So the answer should be 9.9 is bigger than 9.11.\n",
      "</think>\n",
      "\n",
      "9.9ëŠ” 9.11ë³´ë‹¤ ë” í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "**ì´ìœ :**  \n",
      "- 9.9ëŠ” 9.90ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìœ¼ë©°, 9.11ì€ 9.11ë¡œ í‘œí˜„ë©ë‹ˆë‹¤.  \n",
      "- ë¶„ìˆ˜ í˜•íƒœë¡œ ë¹„êµí•˜ë©´, 0.9(9.9)ì€ 0.11(9.11)ë³´ë‹¤ 9ë¶„ì˜ 1ë³´ë‹¤ í° ìˆ˜ì…ë‹ˆë‹¤.  \n",
      "- ë”°ë¼ì„œ 9.9 > 9.11ì…ë‹ˆë‹¤.  \n",
      "\n",
      "**ë‹µë³€:** 9.9ê°€ ë” í° ìˆ˜ì…ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "\n",
    "#model = ChatOllama(model=\"exaone3.5:2.4b\", temperature=0.5)\n",
    "# model = ChatOllama(model=\"qwen2.5:1.5b\", temperature=0.5)\n",
    "model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.1)\n",
    "\n",
    "answer = []\n",
    "for chunk in model.stream(\"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"):\n",
    "    answer.append(chunk.content)\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b751180",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_md=''.join([i for i in answer])\n",
    "display(Markdown(answer_md))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d106fa",
   "metadata": {},
   "source": [
    "### LangGraphë¥¼ ì‚¬ìš©í•˜ì—¬ DeepSeek ëª¨ë¸ê³¼ Qwen ëª¨ë¸ì„ ì—°ë™í•˜ê¸° \n",
    "++ deepseek ëª¨ë¸ì€ ì¶”ë¡ ì„ qwenëª¨ë¸ì€ í•œê¸€ì‘ë‹µì„ ìœ„í•˜ì—¬ ì—°ë™\n",
    "* poetry add langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de1075c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model='deepseek-r1:1.5b' temperature=0.0 stop=['</think>']\n",
      "model='qwen3:1.7b' temperature=0.5\n",
      "input_variables=['question', 'thinking'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='\\n        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\\n\\n        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\\n        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\\n        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\\n        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\\n        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\\n\\n        ì§€ì¹¨:\\n        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\\n        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\\n        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\\n        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\\n\\n        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\\n        '), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'thinking'], input_types={}, partial_variables={}, template='\\n        ì§ˆë¬¸: {question}\\n        ì¶”ë¡ : {thinking}\\n        '), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "#ì¶”ë¡  ëª¨ë¸\n",
    "reasoning_model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0, stop=[\"</think>\"])\n",
    "print(reasoning_model)\n",
    "\n",
    "#í•œê¸€ë¡œ ì‘ë‹µì„ í•˜ê¸° ìœ„í•œ ëª¨ë¸ \n",
    "generation_model = ChatOllama(model=\"qwen3:1.7b\", temperature=0.5)\n",
    "print(generation_model)\n",
    "\n",
    "#LangGraphì—ì„œ State ì‚¬ìš©ìì •ì˜ í´ë˜ìŠ¤ëŠ” ë…¸ë“œ ê°„ì˜ ì •ë³´ë¥¼ ì „ë‹¬í•˜ëŠ” í‹€ì…ë‹ˆë‹¤. \n",
    "#ë…¸ë“œ ê°„ì— ê³„ì† ì „ë‹¬í•˜ê³  ì‹¶ê±°ë‚˜, ê·¸ë˜í”„ ë‚´ì—ì„œ ìœ ì§€í•´ì•¼ í•  ì •ë³´ë¥¼ ë¯¸ë¦¬ ì •ì˜í™ë‹ˆë‹¤. \n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    thinking: str\n",
    "    answer: str\n",
    "\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"\n",
    "        ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëª…í™•í•˜ê³  í¬ê´„ì ì¸ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n",
    "\n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "\n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "\n",
    "        ëª©í‘œ: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë©´ì„œ ì¶”ë¡  ê³¼ì •ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨í•œ ì •ë³´ ì œê³µì…ë‹ˆë‹¤.\n",
    "        \"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"\n",
    "        ì§ˆë¬¸: {question}\n",
    "        ì¶”ë¡ : {thinking}\n",
    "        \"\"\"\n",
    "    )\n",
    "])\n",
    "print(answer_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6fdd693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. So first, I need to compare them. Both numbers start with 9, so the whole numbers are the same. But the decimals are different. \n",
      "\n",
      "Wait, 9.9 is the same as 9.90 when considering the decimal places. So if I write 9.9 as 9.90, then comparing 9.90 and 9.11. Now, looking at the tenths place: 9 vs 1. Since 9 is greater than 1, the tenths place determines the value here. So even though the whole numbers are the same, the decimal part makes 9.9 larger.\n",
      "\n",
      "So the conclusion is that 9.9 is bigger than 9.11. But I need to make sure I explain this clearly without using the word \"reasoning\" or the process. Just present the answer in a natural way.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ì€ 9.11ì…ë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ëŠ” 1ë³´ë‹¤ í¬ë¯€ë¡œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” ì»¤ìš”.\n",
      "{'question': '9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?', 'thinking': \"<think>\\nFirst, I need to compare the two numbers: 9.9 and 9.11.\\n\\nBoth numbers have the same whole number part, which is 9.\\n\\nTo make a fair comparison, I'll align their decimal places by writing 9.9 as 9.90.\\n\\nNow, comparing each digit from left to right:\\n\\n- The units place for both numbers is 9.\\n- In the tenths place, 9 has a 9 and 9.11 has a 1.\\n  \\nSince 9 is greater than 1 in the tenths place, 9.90 is larger than 9.11.\\n\\nTherefore, 9.9 is greater than 9.11.\\n\", 'answer': '<think>\\nOkay, let\\'s see. The user is asking which is bigger between 9.9 and 9.11. Hmm. So first, I need to compare them. Both numbers start with 9, so the whole numbers are the same. But the decimals are different. \\n\\nWait, 9.9 is the same as 9.90 when considering the decimal places. So if I write 9.9 as 9.90, then comparing 9.90 and 9.11. Now, looking at the tenths place: 9 vs 1. Since 9 is greater than 1, the tenths place determines the value here. So even though the whole numbers are the same, the decimal part makes 9.9 larger.\\n\\nSo the conclusion is that 9.9 is bigger than 9.11. But I need to make sure I explain this clearly without using the word \"reasoning\" or the process. Just present the answer in a natural way.\\n</think>\\n\\n9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \\në‘ ìˆ«ìëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \\n9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ì€ 9.11ì…ë‹ˆë‹¤.  \\nì†Œìˆ˜ ë¶€ë¶„ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ëŠ” 1ë³´ë‹¤ í¬ë¯€ë¡œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” ì»¤ìš”.'}\n",
      "==> ìƒì„±ëœ ë‹µë³€: \n",
      "\n",
      "<think>\n",
      "Okay, let's see. The user is asking which is bigger between 9.9 and 9.11. Hmm. So first, I need to compare them. Both numbers start with 9, so the whole numbers are the same. But the decimals are different. \n",
      "\n",
      "Wait, 9.9 is the same as 9.90 when considering the decimal places. So if I write 9.9 as 9.90, then comparing 9.90 and 9.11. Now, looking at the tenths place: 9 vs 1. Since 9 is greater than 1, the tenths place determines the value here. So even though the whole numbers are the same, the decimal part makes 9.9 larger.\n",
      "\n",
      "So the conclusion is that 9.9 is bigger than 9.11. But I need to make sure I explain this clearly without using the word \"reasoning\" or the process. Just present the answer in a natural way.\n",
      "</think>\n",
      "\n",
      "9.9ì™€ 9.11 ì¤‘ ë” í° ìˆ«ìëŠ” **9.9**ì…ë‹ˆë‹¤.  \n",
      "ë‘ ìˆ«ìëŠ” ì •ìˆ˜ ë¶€ë¶„(9)ì´ ê°™ìœ¼ë¯€ë¡œ, ì†Œìˆ˜ ë¶€ë¶„ì„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.  \n",
      "9.9ëŠ” 9.90ê³¼ ê°™ìœ¼ë©°, 9.11ì€ 9.11ì…ë‹ˆë‹¤.  \n",
      "ì†Œìˆ˜ ë¶€ë¶„ì—ì„œ 9 vs 1ì„ ë¹„êµí•˜ë©´, 9ëŠ” 1ë³´ë‹¤ í¬ë¯€ë¡œ 9.9ëŠ” 9.11ë³´ë‹¤ ë” ì»¤ìš”.\n"
     ]
    }
   ],
   "source": [
    "#DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±í•©ë‹ˆë‹¤. \n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    response = reasoning_model.invoke(question)\n",
    "    #print(response.content)\n",
    "    return {\"thinking\": response.content}\n",
    "\n",
    "#Qwenë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "def generate(state: State):\n",
    "    messages = answer_prompt.invoke({\"question\": state[\"question\"], \"thinking\": state[\"thinking\"]})\n",
    "    response = generation_model.invoke(messages)\n",
    "    print(response.content)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# ê·¸ë˜í”„ ì»´íŒŒì¼\n",
    "graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "graph_builder.add_edge(START, \"think\")\n",
    "graph = graph_builder.compile()\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°\n",
    "inputs = {\"question\": \"9.9ì™€ 9.11 ì¤‘ ë¬´ì—‡ì´ ë” í°ê°€ìš”?\"}\n",
    "\n",
    "# invoke()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê·¸ë˜í”„ í˜¸ì¶œ\n",
    "result = graph.invoke(inputs)\n",
    "print(result)\n",
    "\n",
    "# ê²°ê³¼ ì¶œë ¥\n",
    "print(\"==> ìƒì„±ëœ ë‹µë³€: \\n\")\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b85bca22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(draw_method=MermaidDrawMethod.API)        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e71df58",
   "metadata": {},
   "source": [
    "### 2ê°œ ëª¨ë¸ì„ ì—°ë™í•œ ì½”ë“œë¥¼ Gradio(UI) ì‚¬ìš©í•´ì„œ ì¢€ë” ì´ì˜ê²Œ í‘œí˜„í•´ë³´ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41676e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import os\n",
    "import sys\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# UTF-8 ì¸ì½”ë”© ê°•ì œ ì„¤ì • (Jupyter ë…¸íŠ¸ë¶ í˜¸í™˜)\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "os.environ['LANG'] = 'ko_KR.UTF-8'\n",
    "os.environ['LC_ALL'] = 'ko_KR.UTF-8'\n",
    "\n",
    "# Jupyter í™˜ê²½ì—ì„œëŠ” reconfigure ëŒ€ì‹  í™˜ê²½ë³€ìˆ˜ë¡œ ì²˜ë¦¬\n",
    "try:\n",
    "    if hasattr(sys.stdout, 'reconfigure') and sys.stdout.encoding != 'utf-8':\n",
    "        sys.stdout.reconfigure(encoding='utf-8')\n",
    "except (AttributeError, OSError):\n",
    "    # Jupyter ë…¸íŠ¸ë¶ì´ë‚˜ ë‹¤ë¥¸ í™˜ê²½ì—ì„œëŠ” íŒ¨ìŠ¤\n",
    "    pass\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •: ë‘ ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ê³¼ ë‹µë³€ ìƒì„±ì„ ìˆ˜í–‰\n",
    "# - reasoning_model: ì¶”ë¡ ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë‚®ìŒ, ì •í™•í•œ ë¶„ì„ìš©)\n",
    "# - generation_model: ë‹µë³€ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” ëª¨ë¸ (ì˜¨ë„ ë†’ìŒ, ì°½ì˜ì  ì‘ë‹µìš©)\n",
    "reasoning_model = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\", \n",
    "    temperature=0, \n",
    "    stop=[\"</think>\"]\n",
    ")\n",
    "\n",
    "generation_model = ChatOllama(\n",
    "    model=\"qwen2.5:1.5b\", \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# ìƒíƒœ(State) ì •ì˜: ê·¸ë˜í”„ì—ì„œ ìƒíƒœë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•œ ë°ì´í„° êµ¬ì¡°\n",
    "class State(TypedDict):\n",
    "    question: str   # ì‚¬ìš©ìì˜ ì§ˆë¬¸\n",
    "    thinking: str   # ì¶”ë¡  ê²°ê³¼\n",
    "    answer: str     # ìµœì¢… ë‹µë³€\n",
    "\n",
    "# ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿\n",
    "answer_prompt = ChatPromptTemplate([\n",
    "    (\n",
    "        \"system\",\n",
    "        \"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ë¡œ ì‘ë‹µí•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. \n",
    "        ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ë‹¹ì‹ ì˜ ì‘ì—…:\n",
    "        - ì§ˆë¬¸ê³¼ ì œê³µëœ ì¶”ë¡ ì„ ì‹ ì¤‘í•˜ê²Œ ë¶„ì„í•˜ì„¸ìš”.\n",
    "        - ì¶”ë¡ ì—ì„œ ì–»ì€ í†µì°°ë ¥ì„ í¬í•¨í•˜ì—¬ ì˜ êµ¬ì¡°í™”ëœ í•œêµ­ì–´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.\n",
    "        - ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘í•˜ë„ë¡ í•˜ì„¸ìš”.\n",
    "        - ì •ë³´ë¥¼ ëª…í™•í•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ì „ë‹¬í•˜ë˜, ì¶”ë¡  ê³¼ì •ì„ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”.\n",
    "        \n",
    "        ì§€ì¹¨:\n",
    "        - ë‹µë³€ì„ ëŒ€í™” í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ê³ , í¥ë¯¸ë¡­ê²Œ ì „ë‹¬í•˜ì„¸ìš”.\n",
    "        - ì¤‘ìš”í•œ í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë‹¤ë£¨ë©´ì„œë„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.\n",
    "        - ì œê³µëœ ì¶”ë¡ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ê²ƒì„ ì–¸ê¸‰í•˜ì§€ ë§ê³ , ê·¸ í†µì°°ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨ì‹œí‚¤ì„¸ìš”.\n",
    "        - ë„ì›€ì´ ë˜ê³  ì „ë¬¸ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”.\n",
    "        \n",
    "        ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.\"\"\"\n",
    "    ),\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"ì§ˆë¬¸: {question}\n",
    "        \n",
    "        ì¶”ë¡  ê³¼ì •: {thinking}\n",
    "        \n",
    "        ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:\"\"\"\n",
    "    )\n",
    "])\n",
    "\n",
    "\n",
    "def ensure_utf8_string(text):\n",
    "    \"\"\"ë¬¸ìì—´ì´ UTF-8ë¡œ ì œëŒ€ë¡œ ì¸ì½”ë”©ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³€í™˜\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    if isinstance(text, bytes):\n",
    "        try:\n",
    "            return text.decode('utf-8')\n",
    "        except UnicodeDecodeError:\n",
    "            return text.decode('utf-8', errors='ignore')\n",
    "    \n",
    "    # ë¬¸ìì—´ì´ì§€ë§Œ ì¸ì½”ë”© ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆëŠ” ê²½ìš° ì²˜ë¦¬\n",
    "    if isinstance(text, str):\n",
    "        try:\n",
    "            # ë¬¸ìì—´ì„ UTF-8ë¡œ ì¸ì½”ë”©í–ˆë‹¤ê°€ ë‹¤ì‹œ ë””ì½”ë”©í•˜ì—¬ ì •ë¦¬\n",
    "            return text.encode('utf-8').decode('utf-8')\n",
    "        except (UnicodeEncodeError, UnicodeDecodeError):\n",
    "            return text\n",
    "    \n",
    "    return str(text)\n",
    "\n",
    "# DeepSeekë¥¼ í†µí•´ì„œ ì¶”ë¡  ë¶€ë¶„ê¹Œì§€ë§Œ ìƒì„±\n",
    "def think(state: State):\n",
    "    question = state[\"question\"]\n",
    "    print(f\"[DEBUG] ì…ë ¥ ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] ì§ˆë¬¸ íƒ€ì…: {type(question)}\")\n",
    "    \n",
    "    response = reasoning_model.invoke(question)\n",
    "    thinking_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ê¸¸ì´: {len(thinking_content)}\")\n",
    "    print(f\"[DEBUG] ì¶”ë¡  ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°: {thinking_content[:200]}...\")\n",
    "    \n",
    "    return {\"thinking\": thinking_content}\n",
    "\n",
    "# qwen2.5ë¥¼ í†µí•´ì„œ ê²°ê³¼ ì¶œë ¥ ë¶€ë¶„ì„ ìƒì„±\n",
    "def generate(state: State):\n",
    "    question = ensure_utf8_string(state[\"question\"])\n",
    "    thinking = ensure_utf8_string(state[\"thinking\"])\n",
    "    \n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì§ˆë¬¸: {question}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ê¸¸ì´: {len(thinking)}\")\n",
    "    print(f\"[DEBUG] generate í•¨ìˆ˜ - ì¶”ë¡  ë¯¸ë¦¬ë³´ê¸°: {thinking[:200]}...\")\n",
    "    \n",
    "    messages = answer_prompt.invoke({\n",
    "        \"question\": question, \n",
    "        \"thinking\": thinking\n",
    "    })\n",
    "    \n",
    "    print(f\"[DEBUG] í”„ë¡¬í”„íŠ¸ ë©”ì‹œì§€ ìƒì„± ì™„ë£Œ\")\n",
    "    \n",
    "    response = generation_model.invoke(messages)\n",
    "    answer_content = ensure_utf8_string(response.content)\n",
    "    \n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ íƒ€ì…: {type(response.content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ê¸¸ì´: {len(answer_content)}\")\n",
    "    print(f\"[DEBUG] ìµœì¢… ì‘ë‹µ ë‚´ìš©: {answer_content}\")\n",
    "    \n",
    "    return {\"answer\": answer_content}\n",
    "\n",
    "# ê·¸ë˜í”„ ìƒì„± í•¨ìˆ˜: ìƒíƒœ(State) ê°„ì˜ íë¦„ì„ ì •ì˜\n",
    "def create_graph():\n",
    "    graph_builder = StateGraph(State).add_sequence([think, generate])\n",
    "    graph_builder.add_edge(START, \"think\")\n",
    "    return graph_builder.compile()\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„± ë° ì‹¤í–‰\n",
    "def chatbot_interface(message, history):\n",
    "    graph = create_graph()\n",
    "    inputs = {\"question\": message}\n",
    "    result = graph.invoke(inputs)\n",
    "    return result[\"answer\"]\n",
    "\n",
    "iface = gr.ChatInterface(fn=chatbot_interface, title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "\n",
    "# Gradio ì¸í„°í˜ì´ìŠ¤ ì„¤ì •\n",
    "def launch_gradio():\n",
    "    iface = gr.Interface(fn=chatbot_interface, inputs=\"text\", outputs=\"text\", title=\"AI ì±—ë´‡\", description=\"ì§ˆë¬¸ì„ ì…ë ¥í•˜ë©´ AIê°€ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.\")\n",
    "    iface.launch()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #iface.launch()\n",
    "    launch_gradio()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain-app-main-JUdlSi0d-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
