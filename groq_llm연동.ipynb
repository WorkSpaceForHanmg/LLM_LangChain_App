{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68eab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b429d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6698dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe은 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179f3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x000001C4C9176F00> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001C4C968A780> root_client=<openai.OpenAI object at 0x000001C4C9050200> root_async_client=<openai.AsyncOpenAI object at 0x000001C4C9235CA0> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df643648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 LangChain을 기반으로 구축된 오픈 소스 라이브러리로서, 자연어 처리(NLP) 애플리케이션을 쉽게 배포하고 관리할 수 있도록 설계되었습니다. LangChain은 다양한 자연어 처리 모델과 기술을 통합하여 보다 쉽게 NLP 애플리케이션을 개발할 수 있도록 지원하는 프레임워크입니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **모델 서버**: LangServe는 다양한 NLP 모델을 서버에 배포하고 관리할 수 있는 기능을 제공합니다. 이를 통해 개발자는 모델을 쉽게 배포하고, 여러 애플리케이션에서 모델을 공유하고 사용할 수 있습니다.\\n\\n2. **API 제공**: LangServe는 모델을 API로 노출하여 다른 애플리케이션에서 쉽게 사용할 수 있도록 합니다. 이는 마이크로서비스 아키텍처에서 특히 유용합니다.\\n\\n3. **모델 관리**: LangServe는 모델의 버전 관리를 지원합니다. 이를 통해 모델의 업데이트나 변경 사항이 있을 경우, 이전 버전으로 롤백하거나 새로운 버전을 배포하는 것이 쉬워집니다.\\n\\n4. **모니터링 및 로깅**: LangServe는 모델의 성능을 모니터링하고 로그를 기록하는 기능을 제공합니다. 이를 통해 모델의 성능을 추적하고, 문제가 발생했을 때 빠르게 진단하고 해결할 수 있습니다.\\n\\n5. **확장성**: LangServe는 수평 확장성을 지원합니다. 즉, 수요가 증가하면 서버를 쉽게 추가하여 처리량을 늘릴 수 있습니다.\\n\\n6. **다양한 모델 지원**: LangServe는 다양한 NLP 모델과 프레임워크를 지원합니다. 예를 들어, Hugging Face의 Transformers, spaCy, Stanford CoreNLP 등 다양한 모델과 라이브러리를 사용할 수 있습니다.\\n\\nLangServe를 사용하는 주요 이점은 다음과 같습니다:\\n\\n- **빠른 배포**: LangServe를 사용하면 NLP 모델을 빠르게 배포하고 관리할 수 있습니다.\\n- **쉬운 모델 공유**: LangServe는 팀 내에서 또는 외부 파트너와 모델을 쉽게 공유할 수 있는 방법을 제공합니다.\\n- **유연성**: 다양한 모델과 라이브러리를 지원하므로, 개발자는 자신의 필요에 가장 적합한 도구를 선택할 수 있습니다.\\n\\n결론적으로, LangServe는 자연어 처리 애플리케이션을 개발하고 배포하는 과정을 간소화하는 강력한 도구입니다. 개발자가 NLP 모델을 더 쉽게 관리하고 확장할 수 있도록 지원하여, 보다 효율적인 애플리케이션 개발과 운영을 가능하게 합니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 476, 'prompt_tokens': 30, 'total_tokens': 506, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 2.460914881, 'prompt_time': 0.003253317, 'completion_time': 1.187136897, 'total_time': 1.190390214}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-378bbe7d-b681-42ac-99e7-75a5034bc0cb', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--44a3b8b8-dce3-4891-9160-49530819f0f8-0' usage_metadata={'input_tokens': 30, 'output_tokens': 476, 'total_tokens': 506, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 LangChain을 기반으로 구축된 오픈 소스 라이브러리로서, 자연어 처리(NLP) 애플리케이션을 쉽게 배포하고 관리할 수 있도록 설계되었습니다. LangChain은 다양한 자연어 처리 모델과 기술을 통합하여 보다 쉽게 NLP 애플리케이션을 개발할 수 있도록 지원하는 프레임워크입니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **모델 서버**: LangServe는 다양한 NLP 모델을 서버에 배포하고 관리할 수 있는 기능을 제공합니다. 이를 통해 개발자는 모델을 쉽게 배포하고, 여러 애플리케이션에서 모델을 공유하고 사용할 수 있습니다.\n",
      "\n",
      "2. **API 제공**: LangServe는 모델을 API로 노출하여 다른 애플리케이션에서 쉽게 사용할 수 있도록 합니다. 이는 마이크로서비스 아키텍처에서 특히 유용합니다.\n",
      "\n",
      "3. **모델 관리**: LangServe는 모델의 버전 관리를 지원합니다. 이를 통해 모델의 업데이트나 변경 사항이 있을 경우, 이전 버전으로 롤백하거나 새로운 버전을 배포하는 것이 쉬워집니다.\n",
      "\n",
      "4. **모니터링 및 로깅**: LangServe는 모델의 성능을 모니터링하고 로그를 기록하는 기능을 제공합니다. 이를 통해 모델의 성능을 추적하고, 문제가 발생했을 때 빠르게 진단하고 해결할 수 있습니다.\n",
      "\n",
      "5. **확장성**: LangServe는 수평 확장성을 지원합니다. 즉, 수요가 증가하면 서버를 쉽게 추가하여 처리량을 늘릴 수 있습니다.\n",
      "\n",
      "6. **다양한 모델 지원**: LangServe는 다양한 NLP 모델과 프레임워크를 지원합니다. 예를 들어, Hugging Face의 Transformers, spaCy, Stanford CoreNLP 등 다양한 모델과 라이브러리를 사용할 수 있습니다.\n",
      "\n",
      "LangServe를 사용하는 주요 이점은 다음과 같습니다:\n",
      "\n",
      "- **빠른 배포**: LangServe를 사용하면 NLP 모델을 빠르게 배포하고 관리할 수 있습니다.\n",
      "- **쉬운 모델 공유**: LangServe는 팀 내에서 또는 외부 파트너와 모델을 쉽게 공유할 수 있는 방법을 제공합니다.\n",
      "- **유연성**: 다양한 모델과 라이브러리를 지원하므로, 개발자는 자신의 필요에 가장 적합한 도구를 선택할 수 있습니다.\n",
      "\n",
      "결론적으로, LangServe는 자연어 처리 애플리케이션을 개발하고 배포하는 과정을 간소화하는 강력한 도구입니다. 개발자가 NLP 모델을 더 쉽게 관리하고 확장할 수 있도록 지원하여, 보다 효율적인 애플리케이션 개발과 운영을 가능하게 합니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece72a6",
   "metadata": {},
   "source": [
    "### LCEL \n",
    "+ Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8e1ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 설명해주세요.\\n    ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 설명해주세요.\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79bfe695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt + llm 연결결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b4c324",
   "metadata": {},
   "source": [
    "### LCEL \n",
    "+ Prompt + LLM + OutputParser을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb9ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL)     prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66da60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 다양한 AI 관련 제품을 제공하는 회사입니다. LangChain의 주요 제품은 다음과 같습니다:\n",
      "\n",
      "1. **LangServe**: LangServe는 LangChain에서 제공하는 핵심 제품 중 하나입니다. 이것은 LLM(Large Language Model) 기반의 API를 쉽게 구축하고 배포할 수 있도록 지원하는 플랫폼입니다. LangServe를 사용하면 개발자들이 언어 모델을 활용하여 자연어 처리 작업을 보다 효율적으로 수행할 수 있습니다.\n",
      "\n",
      "2. **LangChain SDK**: LangChain SDK는 개발자들이 LangChain의 기능을 활용하여 자신만의 애플리케이션을 구축할 수 있도록 지원하는 소프트웨어 개발 키트입니다. 이 SDK를 통해 개발자들은 다양한 언어 모델과 자연어 처리 기능을 쉽게 통합할 수 있습니다.\n",
      "\n",
      "3. **LangChain Studio**: LangChain Studio는 LangChain에서 제공하는 또 다른 중요한 제품입니다. 이것은 개발자들이 LangChain 기반의 애플리케이션을 보다 쉽게 개발하고 테스트할 수 있도록 지원하는 통합 개발 환경(IDE)입니다.\n",
      "\n",
      "이러한 제품들은 LangChain이 목표로 하는 개방적이고 확장 가능한 AI 생태계를 구축하는 데 중요한 역할을 합니다. LangChain의 제품들은 개발자들이 AI 기술을 보다 쉽게 접근하고 활용할 수 있도록 설계되었습니다.\n",
      "\n",
      "특히, LangServe는 LangChain 제품군 중에서 매우 중요한 위치를 차지하고 있습니다. LangServe를 통해 사용자는 사전에 구축된 다양한 언어 모델을 활용할 수 있으며, 이를 통해 자연어 이해, 자연어 생성, 대화형 AI 등 다양한 자연어 처리 작업을 보다 효율적으로 수행할 수 있습니다.\n",
      "\n",
      "LangChain의 이러한 제품들은 기업이나 개발자들이 AI 기술을 활용하여 혁신적인 서비스를 개발하고, 자연어 처리 분야에서 효율성과 효과를 극대화할 수 있도록 지원합니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda611b",
   "metadata": {},
   "source": [
    "### Runnable의 stream() 함수 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31b4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스트리밍 출력을 위한 요청\n",
    "try:\n",
    "    answer = chain2.stream({\"input\": \"인공지능 모델의 학습 원리를 자세하게 설명해 주세요.\"})\n",
    "    \n",
    "    # 스트리밍 출력\n",
    "    # print(answer)\n",
    "    for token in answer:\n",
    "        # 스트림에서 받은 데이터의 내용을 출력합니다. 줄바꿈 없이 이어서 출력하고, 버퍼를 즉시 비웁니다.\n",
    "        print(token, end=\"\", flush=True)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"오류 발생 : {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e837313",
   "metadata": {},
   "source": [
    "### Multi Chain\n",
    "* 첫번째 Chain의 출력이, 두번째 Chain의 입력이 된다.\n",
    "* 두개의 Chain과 Prompt + OutputParser를 LCEL로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1137c222",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Step 1: 사용자가 입력한 장르에 따라 영화 추천\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{genre} 장르에서 추천할 한국 영화를 한 편 알려주세요.\")\n",
    "\n",
    "# Step 2: 추천된 영화의 줄거리를 요약\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{movie} 추전한 영화의 제목을 먼저 알려주시고, 줄을 바꾸어서 영화의 줄거리를 10문장으로 요약해 주세요.\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "# 체인 1: 영화 추천 (입력: 장르 → 출력: 영화 제목)\n",
    "chain1 = prompt1 | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60c78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체인 2: 줄거리 요약 (입력: 영화 제목 → 출력: 줄거리)\n",
    "try:\n",
    "    chain2 = (\n",
    "        {\"movie\": chain1}  # chain1의 출력을 movie 변수로 전달\n",
    "        | prompt2\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 실행: \"SF\" 장르의 영화 추천 및 줄거리 요약\n",
    "    response = chain2.invoke({\"genre\": \"액션\"})\n",
    "    print(response)  \n",
    "except Exception as e:\n",
    "    print(f\"에러 발생 : {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406df877",
   "metadata": {},
   "source": [
    "### PromptTemplate 여러개 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd022aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "# 템플릿에 값을 채워서 프롬프트를 완성\n",
    "filled_prompt = prompt_template.format(model_name=\"ChatGPT\", count=3)\n",
    "\n",
    "# 문자열 템플릿 결합 (PromptTemplate + PromptTemplate + 문자열)\n",
    "combined_prompt = (\n",
    "              prompt_template\n",
    "              + PromptTemplate.from_template(\"\\n\\n 그리고 {model_name} 모델의 장점을 요약 정리해 주세요\")\n",
    "              + \"\\n\\n {model_name} 모델과 비슷한 AI 모델은 어떤 것이 있나요? 모델명은 {language}로 답변해 주세요.\"\n",
    ")\n",
    "combined_prompt.format(model_name=\"ChatGPT\", count=3, language=\"영어\")\n",
    "\n",
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = combined_prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"model_name\":\"ChatGPT\", \"count\":3, \"language\":\"영어\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b53d84b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GPT-4 모델의 학습 원리를 2 문장으로 요약해서 한국어로 답변해 주세요.', 'Gemma 모델의 학습 원리를 3 문장으로 요약해서 한국어로 답변해 주세요.', 'llama-4 모델의 학습 원리를 4 문장으로 요약해서 한국어로 답변해 주세요.']\n"
     ]
    }
   ],
   "source": [
    "template_text = \"{model_name} 모델의 학습 원리를 {count} 문장으로 요약해서 한국어로 답변해 주세요.\"\n",
    "\n",
    "# PromptTemplate 인스턴스를 생성\n",
    "prompt_template = PromptTemplate.from_template(template_text)\n",
    "\n",
    "questions = [\n",
    "    {\"model_name\": \"GPT-4\", \"count\": 2},\n",
    "    {\"model_name\": \"Gemma\", \"count\": 3},\n",
    "    {\"model_name\": \"llama-4\", \"count\": 4},\n",
    "]\n",
    "\n",
    "# 여러 개의 프롬프트를 미리 생성\n",
    "formatted_prompts = [prompt_template.format(**q) for q in questions]\n",
    "print(formatted_prompts)  # 미리 생성된 질문 목록 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ef296e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-4 모델은 대규모의 텍스트 데이터를 학습하여 언어 패턴과 관계를 파악하고, 이를 바탕으로 주어진 입력에 대해 예측되는 다음 텍스트를 생성하는 방식으로 작동합니다. 이러한 학습 과정은 자기 지도 학습(self-supervised learning) 방식으로 진행되며, 모델이 스스로 데이터를 분석하고 학습할 수 있도록 합니다.\n",
      "Gemma는 컴퓨터가 자연어 처리를 더 잘하도록 설계된 언어 모델입니다. 대규모의 텍스트 데이터를 학습하여 언어의 패턴과 구조를 이해하고, 이를 기반으로 질문에 답변하거나 문장을 생성하는 등의 작업을 수행할 수 있습니다. Gemma는 사전 학습된 모델을 미세 조정하여 특정 작업에 맞게 조정할 수 있으므로, 챗봇, 언어 번역, 텍스트 요약과 같은 다양한 자연어 처리 작업에 활용될 수 있습니다.\n",
      "llama-4 모델은 메타에서 개발한 대규모 언어 모델입니다. 이 모델은 방대한 텍스트 데이터를 기반으로 학습되며, 이를 통해 자연어 처리 능력을 습득합니다. 학습 과정에서 모델은 주어진 문맥에서 다음에 올 단어를 예측하도록 훈련되며, 이 과정을 통해 언어의 패턴과 구조를 학습합니다. 이를 통해 llama-4 모델은 다양한 자연어 처리 작업에 활용될 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt)    #AImessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c38d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI 모델 사용\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "for prompt in formatted_prompts:\n",
    "    response = llm.invoke(prompt) #AIMessage\n",
    "    print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d7eb38",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "* SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3a8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate\n",
    ")\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 개별 메시지 템플릿 정의\n",
    "system_message = SystemMessagePromptTemplate.from_template(\n",
    "    \"당신은 {topic} 전문가입니다. 명확하고 자세하게 한국어로 설명해 주세요.\"\n",
    ")\n",
    "user_message = HumanMessagePromptTemplate.from_template(\n",
    "    \"{question}\"\n",
    ")\n",
    "ai_message = AIMessagePromptTemplate.from_template(\n",
    "    \"This is an example answer about {topic}.\"\n",
    ")\n",
    "\n",
    "# ChatPromptTemplate로 메시지들을 묶기\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_message,\n",
    "    user_message,\n",
    "    ai_message\n",
    "])\n",
    "\n",
    "# 메시지 생성\n",
    "messages = chat_prompt.format_messages(topic=\"AI\", question=\"딥러닝은 무엇인가요?\")\n",
    "\n",
    "# LLM 호출\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "response = llm.invoke(messages)\n",
    "\n",
    "# 결과 출력\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a7e195",
   "metadata": {},
   "source": [
    "### FewShotPromptTemplate\n",
    "* 예시를 제공 프롬프트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3464a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"뉴턴의 운동 법칙을 요약해 주세요.\",\n",
    "        \"output\": \"\"\"### 뉴턴의 운동 법칙\n",
    "1. **관성의 법칙**: 힘이 작용하지 않으면 물체는 계속 같은 상태를 유지합니다.\n",
    "2. **가속도의 법칙**: 물체에 힘이 작용하면, 힘과 질량에 따라 가속도가 결정됩니다.\n",
    "3. **작용-반작용 법칙**: 모든 힘에는 크기가 같고 방향이 반대인 힘이 작용합니다.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"지구의 대기 구성 요소를 알려주세요.\",\n",
    "        \"output\": \"\"\"### 지구 대기의 구성\n",
    "- **질소 (78%)**: 대기의 대부분을 차지합니다.\n",
    "- **산소 (21%)**: 생명체가 호흡하는 데 필요합니다.\n",
    "- **아르곤 (0.93%)**: 반응성이 낮은 기체입니다.\n",
    "- **이산화탄소 (0.04%)**: 광합성 및 온실 효과에 중요한 역할을 합니다.\"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 예제 프롬프트 템플릿\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# FewShotChatMessagePromptTemplate 적용\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "\n",
    "# 최종 프롬프트 구성\n",
    "final_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"당신은 초등학생도 이해할 수 있도록 쉽게 설명하는 과학 교육자입니다.\"),\n",
    "        few_shot_prompt,\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 생성 및 체인 구성\n",
    "#model = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "model = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.7\n",
    ")\n",
    "chain = final_prompt | model\n",
    "\n",
    "# 테스트 실행\n",
    "result = chain.invoke({\"input\": \"태양계의 행성들을 간략히 정리해 주세요.\"})\n",
    "#result = chain.invoke({\"input\": \"양자 얽힘이 무엇인가요?\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eb767",
   "metadata": {},
   "source": [
    "### PartialPromptTemplate\n",
    "* 프롬프트의 입력 값에 함수 호출한 결과 이나 외부 API를 호출한 동적인 값을 대입할 수 있음음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9f2eec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 프롬프트: 겨울에 일어나는 대표적인 지구과학 현상은 태풍 발생입니다.\n",
      "🔹 모델 응답: 겨울에 태풍이 발생하는 것은 다소 예외적인 현상이지만, 지구과학적 관점에서 설명할 수 있습니다.\n",
      "\n",
      "태풍은 일반적으로 따뜻한 해양 수면에서 발생하는 열대성 저기압입니다. 대부분의 태풍은 여름과 초가을에 발생하지만, 겨울에도 예외적으로 태풍이 발생하는 경우가 있습니다.\n",
      "\n",
      "겨울에 태풍이 발생하는 주요 요인은 다음과 같습니다.\n",
      "\n",
      "1.  **이상 고온**: 겨울철에 해양 수온이 평년보다 높을 때 발생할 수 있습니다. 수온이 높은 해양에서는 열대성 저기압이 발생하기 쉬운 조건이 만들어집니다.\n",
      "\n",
      "2.  **기상 이변**: 지구온난화 등 기후변화로 인해 기상 이변이 증가하고 있습니다. 이로 인해 겨울철에 태풍이 발생하는 경우도 있습니다.\n",
      "\n",
      "겨울에 태풍이 발생하면 한반도에 많은 피해를 줄 수 있으므로, 기상청의 예보를 주기적으로 확인하시길 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 계절을 결정하는 함수 (남반구/북반구 고려)\n",
    "def get_current_season(hemisphere=\"north\"):\n",
    "    month = datetime.now().month\n",
    "    \n",
    "    if hemisphere == \"north\":  # 북반구 (기본값)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"봄\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"여름\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"가을\"\n",
    "        else:\n",
    "            return \"겨울\"\n",
    "    else:  # 남반구 (계절 반대)\n",
    "        if 3 <= month <= 5:\n",
    "            return \"가을\"\n",
    "        elif 6 <= month <= 8:\n",
    "            return \"겨울\"\n",
    "        elif 9 <= month <= 11:\n",
    "            return \"봄\"\n",
    "        else:\n",
    "            return \"여름\"\n",
    "\n",
    "# 프롬프트 템플릿 정의 (부분 변수 적용)\n",
    "prompt = PromptTemplate(\n",
    "    template=\"{season}에 일어나는 대표적인 지구과학 현상은 {phenomenon}입니다.\",\n",
    "    input_variables=[\"phenomenon\"],  # 사용자 입력 필요\n",
    "    partial_variables={\"season\": get_current_season(\"south\")}  # 동적으로 계절 값 할당\n",
    ")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "# 특정 계절의 현상 질의\n",
    "query = prompt.format(phenomenon=\"태풍 발생\")  # '태풍 발생'은 여름과 관련됨\n",
    "result = model.invoke(query)\n",
    "\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"🔹 프롬프트: {query}\")\n",
    "print(f\"🔹 모델 응답: {result.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aab046d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=[] input_types={} partial_variables={'info': '1달러 = 1365.14원'} template='현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후에 환율예상값에 대한 분석을 제공해 주세요.'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 실시간 환율을 가져오는 함수\n",
    "def get_exchange_rate():\n",
    "    response = requests.get(\"https://api.exchangerate-api.com/v4/latest/USD\")\n",
    "    data = response.json()\n",
    "    return f\"1달러 = {data['rates']['KRW']}원\"\n",
    "\n",
    "# {info} 변수에 API에서 받은 환율 정보를 동적으로 반영\n",
    "prompt = PromptTemplate(\n",
    "    template=\"현재 {info} 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후에 환율예상값에 대한 분석을 제공해 주세요.\",\n",
    "    input_variables=[],  # 사용자 입력 없음\n",
    "    partial_variables={\"info\": get_exchange_rate()}  # API에서 가져온 데이터 자동 반영\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b449e18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 프롬프트: 현재 1달러 = 1365.14원 기준으로 환율 정보를 알려드립니다. 현재 환율을 기준으로 한국경제에 미치는 영향 및 향후에 환율예상값에 대한 분석을 제공해 주세요.\n",
      "🔹 모델 응답: ## 현재 환율 정보\n",
      "\n",
      "*   현재 1달러 = 1,365.14원\n",
      "\n",
      "## 한국 경제에 미치는 영향\n",
      "\n",
      "### 수출입\n",
      "\n",
      "*   **수출**: 약세 (약달러) → 한국 상품의 가격 경쟁력이 높아져 수출이 증가할 수 있음\n",
      "*   **수입**: 강세 (강달러) → 수입 원자재 가격이 상승하여 국내 산업의 비용 부담이 증가할 수 있음\n",
      "\n",
      "### 물가\n",
      "\n",
      "*   **물가 상승**: 원달러 환율이 상승하면 수입 물가가 상승하여 국내 물가에도 영향을 미칠 수 있음\n",
      "*   **물가 안정**: 원달러 환율이 하락하면 수입 물가가 안정되어 국내 물가가 안정될 수 있음\n",
      "\n",
      "### 금리\n",
      "\n",
      "*   **금리 인상**: 높은 환율은 금리 인상 압력을 가중시킬 수 있음. 높은 금리는 소비와 투자를 위축시켜 경제 성장에 부정적인 영향을 미칠 수 있음\n",
      "*   **금리 인하**: 낮은 환율은 금리 인하를 가능하게 할 수 있음. 낮은 금리는 소비와 투자를 촉진시켜 경제 성장에 긍정적인 영향을 미칠 수 있음\n",
      "\n",
      "### 외국인 투자\n",
      "\n",
      "*   **외국인 투자 증가**: 약세 (약달러) → 한국 자산에 대한 외국인 투자가 증가할 수 있음\n",
      "*   **외국인 투자 감소**: 강세 (강달러) → 한국 자산에 대한 외국인 투자가 감소할 수 있음\n",
      "\n",
      "## 향후 환율 예상값 분석\n",
      "\n",
      "*   **경제 지표**: 미국의 경제 성장률, 물가 상승률, 고용 지표 등\n",
      "*   **글로벌 경제 상황**: 국제 무역, 원자재 가격, 지정학적 리스크 등\n",
      "*   **통화 정책**: 한국은행과 연방준비제도(Fed)의 통화 정책 결정\n",
      "\n",
      "현재 1달러 = 1,365.14원인 상황에서, 향후 환율 예상값을 예측하기 위해서는 다양한 요인을 고려해야 합니다.\n",
      "\n",
      "*   **상승 요인**: 미국의 강한 경제 성장, 고금리 유지, 글로벌 경제 불확실성 증가 등\n",
      "*   **하락 요인**: 한국의 경제 성장률 개선, 수출 증가, 외국인 투자 유치 등\n",
      "\n",
      "최근 경제 상황과 전문가들의 예측에 따르면, 향후 환율은 1,350원 ~ 1,400원 범위에서 움직일 가능성이 있습니다.\n",
      "\n",
      "## 요약\n",
      "\n",
      "*   현재 환율은 1달러 = 1,365.14원이며, 이는 한국 경제에 다양한 영향을 미치고 있음\n",
      "*   향후 환율 예상값은 다양한 요인에 따라 변동할 수 있으며, 전문가들의 예측에 따르면 1,350원 ~ 1,400원 범위에서 움직일 가능성이 있음\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# LLM 모델 설정 \n",
    "model = ChatOpenAI(\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    temperature=0.5\n",
    ")\n",
    "# 모델에 프롬프트 전달 및 응답 받기\n",
    "response = model.invoke(prompt.format())\n",
    "\n",
    "# 결과 출력\n",
    "print(\"🔹 프롬프트:\", prompt.format())\n",
    "print(\"🔹 모델 응답:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
