{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b68eab6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello LangChain\n"
     ]
    }
   ],
   "source": [
    "print('Hello LangChain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b429d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6698dc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='당신은 개발자입니다.'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [ (\"system\", \"당신은 개발자입니다.\") , \n",
    "     (\"user\", \"{input}\") ]\n",
    ")\n",
    "print(prompt)\n",
    "\n",
    "prompt_text = prompt.format(input=\"LangServe은 무엇인가요? 자세하게 설명해주세요\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179f3ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x00000222BC3B6AB0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x00000222BC8CE5D0> root_client=<openai.OpenAI object at 0x00000222B9B49EB0> root_async_client=<openai.AsyncOpenAI object at 0x00000222BC766090> model_name='meta-llama/llama-4-scout-17b-16e-instruct' temperature=0.7 model_kwargs={} openai_api_key=SecretStr('**********') openai_api_base='https://api.groq.com/openai/v1'\n"
     ]
    }
   ],
   "source": [
    "# Groq API를 사용하는 ChatOpenAI 인스턴스 생성\n",
    "llm = ChatOpenAI(\n",
    "    #api_key=OPENAI_API_KEY,\n",
    "    base_url=\"https://api.groq.com/openai/v1\",  # Groq API 엔드포인트\n",
    "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
    "    # model=\"mistral-saba-24b\",\n",
    "    temperature=0.7\n",
    ")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df643648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "content='LangServe는 LangChain을 기반으로 구축된 오픈 소스 라이브러리로서, 자연어 처리(NLP) 모델을 쉽게 배포하고 다른 애플리케이션에 통합할 수 있도록 설계되었습니다. LangChain은 다양한 자연어 처리 모델과 기술을 통합하여 대규모 언어 모델(LLM)을 활용할 수 있는 프레임워크입니다.\\n\\nLangServe의 주요 기능은 다음과 같습니다:\\n\\n1. **모델 배포**: LangServe를 사용하면 NLP 모델을 쉽게 배포하고 관리할 수 있습니다. 모델을 LangServe에 등록하면 API를 통해 모델을 호출하고 결과를 얻을 수 있습니다.\\n\\n2. **모델 통합**: LangServe는 다양한 NLP 모델과 기술을 지원하며, 이를 통해 개발자는 여러 모델을 하나의 플랫폼에서 통합하고 활용할 수 있습니다.\\n\\n3. **확장성**: LangServe는 확장성이 뛰어나며, 대규모 언어 모델을 효율적으로 처리할 수 있습니다.\\n\\n4. **API 지원**: LangServe는 RESTful API를 지원하여 모델을 호출하고 결과를 얻는 데 용이합니다.\\n\\n5. **다양한 모델 지원**: LangServe는 BERT, RoBERTa, LLaMA 등 다양한 NLP 모델을 지원하며, 사용자 정의 모델도 등록할 수 있습니다.\\n\\nLangServe를 사용하면 다음과 같은 이점을 얻을 수 있습니다:\\n\\n* **시간 절약**: LangServe를 사용하면 NLP 모델을 배포하고 관리하는 데 필요한 시간을 절약할 수 있습니다.\\n* **생산성 향상**: LangServe를 사용하면 여러 모델을 하나의 플랫폼에서 통합하고 활용할 수 있으므로 생산성이 향상됩니다.\\n* **확장성**: LangServe는 확장성이 뛰어나므로 대규모 언어 모델을 효율적으로 처리할 수 있습니다.\\n\\nLangServe는 개발자와 연구원이 NLP 모델을 쉽게 배포하고 통합할 수 있도록 설계되었습니다. 따라서 LangServe를 사용하면 NLP 모델을 활용하여 애플리케이션을 개발하는 데 필요한 시간과 노력을 줄일 수 있습니다.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 373, 'prompt_tokens': 30, 'total_tokens': 403, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'queue_time': 0.309639905, 'prompt_time': 0.00301356, 'completion_time': 0.894801447, 'total_time': 0.897815007}, 'model_name': 'meta-llama/llama-4-scout-17b-16e-instruct', 'system_fingerprint': 'fp_79da0e0073', 'id': 'chatcmpl-c85a0ad7-6837-4e0d-828a-81389dc76c6c', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--5f0a4969-de32-49b8-9f99-3b8d445823fe-0' usage_metadata={'input_tokens': 30, 'output_tokens': 373, 'total_tokens': 403, 'input_token_details': {}, 'output_token_details': {}}\n",
      "응답: LangServe는 LangChain을 기반으로 구축된 오픈 소스 라이브러리로서, 자연어 처리(NLP) 모델을 쉽게 배포하고 다른 애플리케이션에 통합할 수 있도록 설계되었습니다. LangChain은 다양한 자연어 처리 모델과 기술을 통합하여 대규모 언어 모델(LLM)을 활용할 수 있는 프레임워크입니다.\n",
      "\n",
      "LangServe의 주요 기능은 다음과 같습니다:\n",
      "\n",
      "1. **모델 배포**: LangServe를 사용하면 NLP 모델을 쉽게 배포하고 관리할 수 있습니다. 모델을 LangServe에 등록하면 API를 통해 모델을 호출하고 결과를 얻을 수 있습니다.\n",
      "\n",
      "2. **모델 통합**: LangServe는 다양한 NLP 모델과 기술을 지원하며, 이를 통해 개발자는 여러 모델을 하나의 플랫폼에서 통합하고 활용할 수 있습니다.\n",
      "\n",
      "3. **확장성**: LangServe는 확장성이 뛰어나며, 대규모 언어 모델을 효율적으로 처리할 수 있습니다.\n",
      "\n",
      "4. **API 지원**: LangServe는 RESTful API를 지원하여 모델을 호출하고 결과를 얻는 데 용이합니다.\n",
      "\n",
      "5. **다양한 모델 지원**: LangServe는 BERT, RoBERTa, LLaMA 등 다양한 NLP 모델을 지원하며, 사용자 정의 모델도 등록할 수 있습니다.\n",
      "\n",
      "LangServe를 사용하면 다음과 같은 이점을 얻을 수 있습니다:\n",
      "\n",
      "* **시간 절약**: LangServe를 사용하면 NLP 모델을 배포하고 관리하는 데 필요한 시간을 절약할 수 있습니다.\n",
      "* **생산성 향상**: LangServe를 사용하면 여러 모델을 하나의 플랫폼에서 통합하고 활용할 수 있으므로 생산성이 향상됩니다.\n",
      "* **확장성**: LangServe는 확장성이 뛰어나므로 대규모 언어 모델을 효율적으로 처리할 수 있습니다.\n",
      "\n",
      "LangServe는 개발자와 연구원이 NLP 모델을 쉽게 배포하고 통합할 수 있도록 설계되었습니다. 따라서 LangServe를 사용하면 NLP 모델을 활용하여 애플리케이션을 개발하는 데 필요한 시간과 노력을 줄일 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = llm.invoke(prompt_text)\n",
    "    print(type(response))\n",
    "    print(response)\n",
    "    print(\"응답:\", response.content)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cece72a6",
   "metadata": {},
   "source": [
    "### LCEL \n",
    "+ Prompt + LLM을 Chain으로 연결하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8e1ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='\\n    You are an expert in AI Expert. Answer the question. \\n    <Question>: {input}에 대해 쉽게 설명해주세요.\\n    ')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    You are an expert in AI Expert. Answer the question. \n",
    "    <Question>: {input}에 대해 쉽게 설명해주세요.\n",
    "    \"\"\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bfe695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "# chain 연결 (LCEL) prompt + llm 연결결\n",
    "chain = prompt | llm\n",
    "print(type(chain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb9ced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableSequence'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# chain 연결 (LCEL)     prompt + llm + outputparser\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain2 = prompt | llm | output_parser\n",
    "print(type(chain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66da60e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "LangChain은 다양한 AI 관련 제품을 제공하는 회사입니다. LangChain의 주요 제품은 다음과 같습니다:\n",
      "\n",
      "1. **LangServe**: LangServe는 LangChain에서 제공하는 핵심 제품 중 하나입니다. LangServe는 대규모 언어 모델(LLM, Large Language Model)을 쉽게 배포하고 관리할 수 있는 플랫폼입니다. LangServe를 사용하면 개발자는 LLM을 자신만의 애플리케이션에 통합할 수 있으며, 모델의 성능을 모니터링하고, 필요에 따라 모델을 업데이트할 수 있습니다.\n",
      "\n",
      "2. **LangChain**: LangChain은 자체 프레임워크입니다. 이 프레임워크는 개발자가 다양한 LLM을 활용하여 애플리케이션을 구축할 수 있도록 지원합니다. LangChain 프레임워크는 모델의 선택, 데이터 처리, 프롬프트 관리 등 LLM 기반 애플리케이션 개발에 필요한 다양한 기능을 제공합니다.\n",
      "\n",
      "3. **LangSmith**: LangSmith는 LangChain에서 제공하는 또 다른 제품입니다. LangSmith는 개발자가 LLM 기반 애플리케이션을 더 쉽게 개발, 테스트, 배포할 수 있도록 돕는 도구입니다. 여기에는 프롬프트 테스트, 모델 평가, 디버깅 등 다양한 기능이 포함되어 있습니다.\n",
      "\n",
      "이러한 제품들은 LangChain이 목표로 하는 개방적이고 유연한 LLM 생태계 구축에 중요한 역할을 합니다. LangChain의 제품들은 모두 개발자가 LLM의 힘을 빌려 혁신적인 애플리케이션을 만들 수 있도록 지원합니다.\n",
      "\n",
      "예를 들어, LangServe를 사용하면 다음과 같은 일을 할 수 있습니다:\n",
      "\n",
      "- **모델 배포**: 다양한 LLM을 선택하고, 이를 애플리케이션에 쉽게 통합할 수 있습니다.\n",
      "- **성능 모니터링**: 배포된 모델의 성능을 실시간으로 모니터링하고, 필요에 따라 모델을 업데이트할 수 있습니다.\n",
      "\n",
      "LangChain과 같은 제품들은 AI 기술의 발전과 그 적용을 가속화하는 데 큰 역할을 하고 있습니다.\n"
     ]
    }
   ],
   "source": [
    "# chain 호출\n",
    "try:\n",
    "    result = chain2.invoke({\"input\": \"LangChain의 Products(제품)는 어떤 것들이 있나요? 예를 들어 LangServe 같은 Product가 있어\"})\n",
    "    print(type(result))\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cae4b5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-basic-kGdHTiMZ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
