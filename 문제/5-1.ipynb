{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595370bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "loader = TextLoader(\"../data/cafe_menu_data.txt\", encoding='utf-8')\n",
    "docs = loader.load()\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "embedding = OllamaEmbeddings(model=\"bge-m3\")\n",
    "db = FAISS.from_documents(splits, embedding)\n",
    "db.save_local(\"./db/cafe_db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f634f64f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3312\\52309614.py:7: LangChainDeprecationWarning: The class `TavilySearchResults` was deprecated in LangChain 0.3.25 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-tavily package and should be used instead. To use it run `pip install -U :class:`~langchain-tavily` and import as `from :class:`~langchain_tavily import TavilySearch``.\n",
      "  tavily = TavilySearchResults(max_results=3)\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.wikipedia import WikipediaAPIWrapper\n",
    "\n",
    "tavily = TavilySearchResults(max_results=3)\n",
    "wiki = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=1000)\n",
    "\n",
    "# cafe DB 불러오기\n",
    "retriever = FAISS.load_local(\"./db/cafe_db\", embedding, allow_dangerous_deserialization=True).as_retriever()\n",
    "\n",
    "@tool\n",
    "def db_search_cafe_func(query: str) -> List[str]:\n",
    "    \"\"\"카페 메뉴 DB에서 메뉴 정보를 검색합니다.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "@tool\n",
    "def tavily_search_func(query: str) -> str:\n",
    "    \"\"\"웹에서 최신 정보를 검색합니다.\"\"\"\n",
    "    return str(tavily.invoke(query))\n",
    "\n",
    "@tool\n",
    "def wiki_summary(query: str) -> str:\n",
    "    \"\"\"위키피디아에서 요약 정보를 검색합니다.\"\"\"\n",
    "    return str(wiki.run(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a189213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")  # 또는 \"gpt-3.5-turbo\"\n",
    "llm_with_tools = llm.bind_tools([db_search_cafe_func, tavily_search_func, wiki_summary])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8498e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "도구 호출: db_search_cafe_func\n",
      "최종 응답:\n",
      "아메리카노는 ₩4,500에 판매되며, 주요 원료는 에스프레소와 뜨거운 물입니다. 진한 에스프레소에 뜨거운 물을 더해 만들어진 클래식한 블랙 커피로, 깔끔하고 깊은 풍미가 특징입니다. 추가로 설탕이나 시럽을 넣어 맞춤형으로 즐길 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "\n",
    "# 사용자 질문 → 아메리카노 정보 요청\n",
    "query = \"아메리카노의 가격과 특징은 무엇인가요?\"\n",
    "\n",
    "# LLM에게 질의 → 어떤 도구 호출할지 판단\n",
    "ai_msg = llm_with_tools.invoke(query)\n",
    "\n",
    "# 도구 호출 정보 확인\n",
    "if ai_msg.tool_calls:\n",
    "    tool_call = ai_msg.tool_calls[0]\n",
    "    print(\"도구 호출:\", tool_call['name'])\n",
    "\n",
    "    # 도구 실행\n",
    "    if tool_call['name'] == 'db_search_cafe_func':\n",
    "        tool_output = db_search_cafe_func.invoke(tool_call['args'])\n",
    "\n",
    "        # 도구 결과를 ToolMessage로 래핑\n",
    "        tool_msg = ToolMessage(\n",
    "            content=str(tool_output),\n",
    "            tool_call_id=tool_call['id'],\n",
    "            name=tool_call['name']\n",
    "        )\n",
    "\n",
    "        # 최종 응답 재요청\n",
    "        final_response = llm_with_tools.invoke(\n",
    "            [HumanMessage(content=query), ai_msg, tool_msg]\n",
    "        )\n",
    "\n",
    "        print(\"최종 응답:\")\n",
    "        print(final_response.content)\n",
    "\n",
    "else:\n",
    "    # 도구 호출 없이 바로 응답\n",
    "    print(\"도구 없이 직접 응답:\", ai_msg.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-langchain-app-main-JUdlSi0d-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
